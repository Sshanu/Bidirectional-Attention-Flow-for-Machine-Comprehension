{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from read_data import read_data, get_squad_data_filter, update_config\n",
    "from tensorflow.contrib.rnn.python.ops.rnn_cell import _linear\n",
    "import flag as fg\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "from my.tensorflow import get_initializer\n",
    "\n",
    "config = fg.main(_)\n",
    "config.out_dir = os.path.join(config.out_base_dir, config.model_name, str(config.run_id).zfill(2))\n",
    "\n",
    "assert config.load or config.mode == 'train', \"config.load must be True if not training\"\n",
    "if not config.load and os.path.exists(config.out_dir):\n",
    "    shutil.rmtree(config.out_dir)\n",
    "\n",
    "config.save_dir = os.path.join(config.out_dir, \"save\")\n",
    "config.log_dir = os.path.join(config.out_dir, \"log\")\n",
    "config.eval_dir = os.path.join(config.out_dir, \"eval\")\n",
    "config.answer_dir = os.path.join(config.out_dir, \"answer\")\n",
    "if not os.path.exists(config.out_dir):\n",
    "    os.makedirs(config.out_dir)\n",
    "if not os.path.exists(config.save_dir):\n",
    "    os.mkdir(config.save_dir)\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.mkdir(config.log_dir)\n",
    "if not os.path.exists(config.answer_dir):\n",
    "    os.mkdir(config.answer_dir)\n",
    "if not os.path.exists(config.eval_dir):\n",
    "    os.mkdir(config.eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERY_BIG_NUMBER = 1e30\n",
    "VERY_SMALL_NUMBER = 1e-30\n",
    "VERY_POSITIVE_NUMBER = VERY_BIG_NUMBER\n",
    "VERY_NEGATIVE_NUMBER = -VERY_BIG_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87507/87599 examples from train\n",
      "Loaded 10544/10570 examples from dev\n"
     ]
    }
   ],
   "source": [
    "data_filter = get_squad_data_filter(config)\n",
    "\n",
    "train_data = read_data(config, 'train', False, data_filter=data_filter)\n",
    "dev_data = read_data(config, 'dev', False, data_filter=data_filter)\n",
    "\n",
    "update_config(config, [train_data, dev_data])\n",
    "\n",
    "word2vec_dict = train_data.shared['lower_word2vec'] if config.lower_word else train_data.shared['word2vec']\n",
    "word2idx_dict = train_data.shared['word2idx']\n",
    "\n",
    "idx2vec_dict = {word2idx_dict[word]: vec for word, vec in word2vec_dict.items() if word in word2idx_dict}\n",
    "emb_mat = np.array([idx2vec_dict[idx] if idx in idx2vec_dict\n",
    "                    else np.random.multivariate_normal(np.zeros(config.word_emb_size), np.eye(config.word_emb_size))\n",
    "                    for idx in range(config.word_vocab_size)])\n",
    "config.emb_mat = emb_mat\n",
    "\n",
    "# pprint(config.__flags, indent=2)\n",
    "\n",
    "# Context and Ques Parameters\n",
    "N = config.batch_size\n",
    "M = config.max_num_sents\n",
    "JX = config.max_sent_size\n",
    "JQ = config.max_ques_size\n",
    "VW = config.word_vocab_size\n",
    "VC = config.char_vocab_size\n",
    "W = config.max_word_size\n",
    "d =  config.hidden_size\n",
    "dc = config.char_emb_size\n",
    "dw = config.word_emb_size\n",
    "dco = config.char_out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "x = tf.placeholder('int32', [N, None, None], name='x')\n",
    "cx = tf.placeholder('int32', [N, None, None, W], name='cx')\n",
    "x_mask = tf.placeholder('bool', [N, None, None], name='x_mask')\n",
    "q = tf.placeholder('int32', [N, None], name='q')\n",
    "cq = tf.placeholder('int32', [N, None, W], name='cq')\n",
    "q_mask = tf.placeholder('bool', [N, None], name='q_mask')\n",
    "y = tf.placeholder('bool', [N, None, None], name='y')\n",
    "y2 = tf.placeholder('bool', [N, None, None], name='y2')\n",
    "is_train = tf.placeholder('bool', [], name='is_train')\n",
    "new_emb_mat = tf.placeholder('float', [None, config.word_emb_size], name='new_emb_mat')\n",
    "\n",
    "global_step = tf.get_variable('global_step', shape=[], dtype='int32', initializer=tf.constant_initializer(0), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_initializer(matrix):\n",
    "    def _initializer(shape, dtype=None, partition_info=None, **kwargs): return matrix\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob, is_train, noise_shape=None, seed=None, name=None):\n",
    "    with tf.name_scope(name or \"dropout\"):\n",
    "        if keep_prob < 1.0:\n",
    "            d = tf.nn.dropout(x, keep_prob, noise_shape=noise_shape, seed=seed)\n",
    "            out = tf.cond(is_train, lambda: d, lambda: x)\n",
    "            return out\n",
    "        return x\n",
    "\n",
    "def conv1d(in_, filter_size, height, padding, is_train=None, keep_prob=1.0, scope=None):\n",
    "    with tf.variable_scope(scope or \"conv1d\"):\n",
    "        num_channels = in_.get_shape()[-1]\n",
    "        filter_ = tf.get_variable(\"filter\", shape=[1, height, num_channels, filter_size], dtype='float')\n",
    "        bias = tf.get_variable(\"bias\", shape=[filter_size], dtype='float')\n",
    "        strides = [1, 1, 1, 1]\n",
    "        if is_train is not None and keep_prob < 1.0:\n",
    "            in_ = dropout(in_, keep_prob, is_train)\n",
    "        xxc = tf.nn.conv2d(in_, filter_, strides, padding) + bias  # [N*M, JX, W/filter_stride, d]\n",
    "        out = tf.reduce_max(tf.nn.relu(xxc), 2)  # [-1, JX, d]\n",
    "        return out\n",
    "\n",
    "def multi_conv1d(in_, filter_sizes, heights, padding, is_train=None, keep_prob=1.0, scope=None):\n",
    "    with tf.variable_scope(scope or \"multi_conv1d\"):\n",
    "        assert len(filter_sizes) == len(heights)\n",
    "        outs = []\n",
    "        for filter_size, height in zip(filter_sizes, heights):\n",
    "            if filter_size == 0:\n",
    "                continue\n",
    "            out = conv1d(in_, filter_size, height, padding, is_train=is_train, keep_prob=keep_prob, scope=\"conv1d_{}\".format(height))\n",
    "            outs.append(out)\n",
    "        concat_out = tf.concat(outs, 2)\n",
    "        return concat_out\n",
    "    \n",
    "def flatten(tensor, keep):\n",
    "    fixed_shape = tensor.get_shape().as_list()\n",
    "    start = len(fixed_shape) - keep\n",
    "    left = reduce(mul, [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start)])\n",
    "    out_shape = [left] + [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start, len(fixed_shape))]\n",
    "    flat = tf.reshape(tensor, out_shape)\n",
    "    return flat\n",
    "\n",
    "def reconstruct(tensor, ref, keep):\n",
    "    ref_shape = ref.get_shape().as_list()\n",
    "    tensor_shape = tensor.get_shape().as_list()\n",
    "    ref_stop = len(ref_shape) - keep\n",
    "    tensor_start = len(tensor_shape) - keep\n",
    "    pre_shape = [ref_shape[i] or tf.shape(ref)[i] for i in range(ref_stop)]\n",
    "    keep_shape = [tensor_shape[i] or tf.shape(tensor)[i] for i in range(tensor_start, len(tensor_shape))]\n",
    "    # pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))]\n",
    "    # keep_shape = tensor.get_shape().as_list()[-keep:]\n",
    "    target_shape = pre_shape + keep_shape\n",
    "    out = tf.reshape(tensor, target_shape)\n",
    "    return out\n",
    "\n",
    "def linear(args, output_size, scope=None, is_train=None, input_keep_prob=1.0):\n",
    "    \n",
    "    flat_args = [flatten(arg, 1) for arg in args]\n",
    "    shape = args[0].get_shape().as_list()\n",
    "    input_size = shape[-1]\n",
    "    \n",
    "    if input_keep_prob < 1.0:\n",
    "        assert is_train is not None\n",
    "        flat_args = [tf.cond(is_train, lambda: tf.nn.dropout(arg, input_keep_prob), lambda: arg)\n",
    "                     for arg in flat_args]\n",
    "        \n",
    "    with tf.variable_scope(scope or \"linear\"):\n",
    "        W = tf.get_variable(\"W\", [output_size, input_size], dtype=args[0].dtype)\n",
    "        b = tf.get_variable(\"b\", [output_size], dtype=args[0].dtype)\n",
    "\n",
    "        flat_out = tf.matmul(flat_args[0], tf.transpose(W)) + b\n",
    "        out = reconstruct(flat_out, args[0], 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "def highway_layer(arg, scope=None, input_keep_prob=1.0, is_train=None):\n",
    "    with tf.variable_scope(scope or \"highway_layer\"):\n",
    "        d = arg.get_shape()[-1]\n",
    "        trans = linear([arg], d, scope='trans', input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "        trans = tf.nn.relu(trans)\n",
    "        gate = linear([arg], d, scope='gate', input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "        gate = tf.nn.sigmoid(gate)\n",
    "        out = gate * trans + (1 - gate) * arg\n",
    "        return out\n",
    "\n",
    "\n",
    "def highway_network(arg, num_layers, input_keep_prob=1.0, is_train=None):\n",
    "    prev = arg\n",
    "    cur = None\n",
    "    for layer_idx in range(num_layers):\n",
    "        cur = highway_layer(prev, scope=\"layer_{}\".format(layer_idx), \n",
    "                            input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "        prev = cur\n",
    "    return cur\n",
    "\n",
    "def mask(val, mask, name=None):\n",
    "    if name is None:\n",
    "        name = 'mask'\n",
    "    return tf.mul(val, tf.cast(mask, 'float'), name=name)\n",
    "\n",
    "\n",
    "def exp_mask(val, mask, name=None):\n",
    "    \"\"\"Give very negative number to unmasked elements in val.\n",
    "    For example, [-3, -2, 10], [True, True, False] -> [-3, -2, -1e9].\n",
    "    Typically, this effectively masks in exponential space (e.g. softmax)\n",
    "    Args:\n",
    "        val: values to be masked\n",
    "        mask: masking boolean tensor, same shape as tensor\n",
    "        name: name for output tensor\n",
    "    Returns:\n",
    "        Same shape as val, where some elements are very small (exponentially zero)\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"exp_mask\"\n",
    "    return tf.add(val, (1 - tf.cast(mask, 'float')) * VERY_NEGATIVE_NUMBER, name=name)\n",
    "\n",
    "def softmax(logits, mask=None, scope=None):\n",
    "    with tf.name_scope(scope or \"Softmax\"):\n",
    "        if mask is not None:\n",
    "            logits = exp_mask(logits, mask)\n",
    "        flat_logits = flatten(logits, 1)\n",
    "        flat_out = tf.nn.softmax(flat_logits)\n",
    "        out = reconstruct(flat_out, logits, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"embedding_layer\"):\n",
    "    if config.use_char_emb:\n",
    "        with tf.variable_scope(\"char\"):\n",
    "\n",
    "            char_emb_mat = tf.get_variable(\"char_emb_mat\", shape=[VC, dc], dtype='float')\n",
    "    \n",
    "            Acx = tf.nn.embedding_lookup(char_emb_mat, cx)  # [N, M, JX, W, dc]\n",
    "            Acq = tf.nn.embedding_lookup(char_emb_mat, cq)  # [N, JQ, W, dc]\n",
    "            Acx = tf.reshape(Acx, [-1, JX, W, dc])\n",
    "            Acq = tf.reshape(Acq, [-1, JQ, W, dc])\n",
    "            \n",
    "            filter_sizes = list(map(int, config.out_channel_dims.split(',')))\n",
    "            heights = list(map(int, config.filter_heights.split(',')))\n",
    "            \n",
    "            with tf.variable_scope(\"conv\"):\n",
    "                xx = multi_conv1d(Acx, filter_sizes, heights, \"VALID\", is_train, config.keep_prob, scope=\"xx\")\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                qq = multi_conv1d(Acq, filter_sizes, heights, \"VALID\", is_train, config.keep_prob, scope=\"xx\")\n",
    "\n",
    "                xx = tf.reshape(xx, [-1, M, JX, dco])\n",
    "                qq = tf.reshape(qq, [-1, JQ, dco])\n",
    "            \n",
    "            \n",
    "    if config.use_word_emb:\n",
    "        with tf.name_scope(\"word\"):\n",
    "            \n",
    "            if config.mode == 'train':\n",
    "                word_emb_mat = tf.get_variable(\"word_emb_mat\", dtype='float', shape=[VW, dw], initializer=get_initializer(config.emb_mat))\n",
    "            else:\n",
    "                word_emb_mat = tf.get_variable(\"word_emb_mat\", shape=[VW, dw], dtype='float')\n",
    "            \n",
    "            if config.use_glove_for_unk:\n",
    "                word_emb_mat = tf.concat([word_emb_mat, new_emb_mat], 0)\n",
    "\n",
    "            Ax = tf.nn.embedding_lookup(word_emb_mat, x)  # [N, M, JX, d]\n",
    "            Aq = tf.nn.embedding_lookup(word_emb_mat, q)  # [N, JQ, d]\n",
    "            \n",
    "        if config.use_char_emb:\n",
    "            xx = tf.concat([xx, Ax], 3)  # [N, M, JX, di]\n",
    "            qq = tf.concat([qq, Aq], 2)  # [N, JQ, di]\n",
    "        else:\n",
    "            xx = Ax\n",
    "            qq = Aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"highway_network_layer\"):\n",
    "    xx = highway_network(xx, config.highway_num_layers, is_train=is_train)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    qq = highway_network(qq, config.highway_num_layers, is_train=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = tf.reduce_sum(tf.cast(x_mask, 'int32'), 2)  # [N, M]\n",
    "q_len = tf.reduce_sum(tf.cast(q_mask, 'int32'), 1)  # [N]\n",
    "\n",
    "flat_len_q = None if q_len is None else tf.cast(flatten(q_len, 0), 'int64')\n",
    "flat_len_x = None if x_len is None else tf.cast(flatten(x_len, 0), 'int64')\n",
    "\n",
    "with tf.variable_scope(\"contextual_layer\"):\n",
    "    cell=tf.nn.rnn_cell.BasicLSTMCell(d,state_is_tuple=True);\n",
    "\n",
    "    flat_qq = flatten(qq, 2)  \n",
    "    (flat_fwu_outputs, flat_bwu_outputs), _ = tf.nn.bidirectional_dynamic_rnn(cell, cell, flat_qq, sequence_length=flat_len_q, dtype='float', scope='lstm')\n",
    "    fw_u = reconstruct(flat_fwu_outputs, qq, 2)\n",
    "    bw_u = reconstruct(flat_bwu_outputs, qq, 2)\n",
    "    u = tf.concat([fw_u, bw_u], 2)\n",
    "\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    flat_xx = flatten(xx, 2)  \n",
    "    (flat_fwh_outputs, flat_bwh_outputs), _ = tf.nn.bidirectional_dynamic_rnn(cell, cell, flat_xx, sequence_length=flat_len_x, dtype='float', scope='lstm')\n",
    "    fw_h = reconstruct(flat_fwh_outputs, xx, 2)\n",
    "    bw_h = reconstruct(flat_bwh_outputs, xx, 2)\n",
    "    h = tf.concat([fw_h, bw_h], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"attention_layer\"):\n",
    "    h_aug = tf.tile(tf.expand_dims(h, 3), [1, 1, 1, JQ, 1])\n",
    "    u_aug = tf.tile(tf.expand_dims(tf.expand_dims(u, 1), 1), [1, M, JX, 1, 1])\n",
    "    h_mask_aug = tf.tile(tf.expand_dims(x_mask, 3), [1, 1, 1, JQ])\n",
    "    u_mask_aug = tf.tile(tf.expand_dims(tf.expand_dims(q_mask, 1), 1), [1, M, JX, 1])\n",
    "    hu_mask = h_mask_aug & u_mask_aug\n",
    "\n",
    "    h_u = h_aug * u_aug\n",
    "\n",
    "    with tf.variable_scope(\"similarity\"):\n",
    "        sim = linear([tf.concat([h_aug, u_aug, h_u], -1)], 1, is_train=is_train, scope=\"sim\")\n",
    "        sim = tf.squeeze(sim, [len(sim.get_shape().as_list())-1])\n",
    "        sim = exp_mask(sim, hu_mask)\n",
    "\n",
    "    with tf.variable_scope(\"context_2_query\"):\n",
    "        a = softmax(sim)\n",
    "        rank_u = len(u_aug.get_shape().as_list())\n",
    "        u_a = tf.reduce_sum(tf.expand_dims(a, -1) * u_aug, rank_u-2)\n",
    "\n",
    "    with tf.variable_scope(\"query_2_context\"):\n",
    "        b = softmax(tf.reduce_max(sim, 3))\n",
    "        rank_h = len(h.get_shape().as_list())\n",
    "        h_a = tf.reduce_sum(tf.expand_dims(b, -1) * h, rank_h-2)\n",
    "        h_a = tf.tile(tf.expand_dims(h_a, 2), [1, 1, JX, 1])\n",
    "    \n",
    "    with tf.variable_scope(\"final\"):\n",
    "        g = tf.concat([h, u_a, h * u_a, h * h_a], 3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"modeling_layer\"):\n",
    "    flat_g = flatten(g, 2)  \n",
    "    cell1 = tf.nn.rnn_cell.BasicLSTMCell(d,state_is_tuple=True);\n",
    "    (flat_fw_g0_outputs, flat_bw_g0_outputs), _ =tf.nn.bidirectional_dynamic_rnn(cell1, cell1, flat_g, sequence_length=flat_len_x, dtype='float', scope='g0')\n",
    "    fw_g0 = reconstruct(flat_fw_g0_outputs, g, 2)\n",
    "    bw_g0 = reconstruct(flat_bw_g0_outputs, g, 2)\n",
    "\n",
    "    g0 = tf.concat([fw_g0, bw_g0], 3)\n",
    "\n",
    "    flat_g0 = flatten(g0, 2)\n",
    "    cell2 = tf.nn.rnn_cell.BasicLSTMCell(d,state_is_tuple=True);\n",
    "\n",
    "    (flat_fw_g1_outputs, flat_bw_g1_outputs), _ =tf.nn.bidirectional_dynamic_rnn(cell2, cell2, flat_g0, sequence_length=flat_len_x, dtype='float', scope='g1')\n",
    "    fw_g1 = reconstruct(flat_fw_g1_outputs, g0, 2)\n",
    "    bw_g1 = reconstruct(flat_bw_g1_outputs, g0, 2)\n",
    "\n",
    "    g1 = tf.concat([fw_g1, bw_g1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"output_layer\"):\n",
    "    logits1 = linear([tf.concat([g1, g], -1)], 1, input_keep_prob=config.input_keep_prob, is_train=is_train, scope=\"logits1\")\n",
    "    logits1 = tf.squeeze(logits1, [len(logits1.get_shape().as_list())-1])\n",
    "    logits1 = exp_mask(logits1, x_mask)\n",
    "    \n",
    "    a = softmax(tf.reshape(logits1, [N, M * JX]))\n",
    "    g1_reshaped = tf.reshape(g1, [N, M * JX, 2 * d])\n",
    "    rank_g1 = len(g1_reshaped.get_shape().as_list())\n",
    "    a1i = tf.reduce_sum(tf.expand_dims(a, -1) * g1_reshaped, rank_g1-2)\n",
    "    a1i = tf.tile(tf.expand_dims(tf.expand_dims(a1i, 1), 1), [1, M, JX, 1])\n",
    "    \n",
    "    g2_input = tf.concat([g, g1, a1i, g1 * a1i], 3)\n",
    "    flat_input = flatten(g2_input, 2)  \n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(d,state_is_tuple=True);\n",
    "    (flat_fw_g2_outputs, flat_bw_g2_outputs), _ =tf.nn.bidirectional_dynamic_rnn(cell, cell, flat_input, sequence_length=flat_len_x, dtype='float', scope='g2')\n",
    "    fw_g2 = reconstruct(flat_fw_g2_outputs, g, 2)\n",
    "    bw_g2 = reconstruct(flat_bw_g2_outputs, g, 2)\n",
    "\n",
    "    g2 = tf.concat([fw_g2, bw_g2], 3)\n",
    "    \n",
    "    logits2 = linear([tf.concat([g2, g], -1)], 1, input_keep_prob=config.input_keep_prob, is_train=is_train, scope=\"logits2\")\n",
    "    logits2 = tf.squeeze(logits2, [len(logits2.get_shape().as_list())-1])\n",
    "    logits2 = exp_mask(logits2, x_mask)\n",
    "    \n",
    "    logits1 = tf.reshape(logits1, [-1, M * JX])\n",
    "    flat_yp1 = tf.nn.softmax(logits1) \n",
    "    yp1 = tf.reshape(flat_yp1, [-1, M, JX])\n",
    "    logits2 = tf.reshape(logits2, [-1, M * JX])\n",
    "    flat_yp2 = tf.nn.softmax(logits2)\n",
    "    yp2 = tf.reshape(flat_yp2, [-1, M, JX])\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
